{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 8 (Optional): Llama Helper Function\n",
    "Here, you'll walk through the code that makes up the llama helper function that you've been using throughout the course, to see how it works.\n",
    "\n",
    "https://learn.deeplearning.ai/courses/prompt-engineering-with-llama-2/lesson/7/code-llama\n",
    "L2_getting_started.ipynb   (chat vs. base models, changing temperature settings, changing max_tokens, askiing follow up question)\n",
    "L3_multi_turn_conversations.ipynb (LLMs are stateless, constructing multi-turn prompts, llama_helper_hchat function, )\n",
    "L4_prompt_engineering_techniques.ipynb (in-context learning, zero-shot prompting, few-shot promting, specifying output format, role prompting, summarization, providing new info in the prompt, chain of thought prompting)\n",
    "L5_comparing_llama_models.ipynb (sentiment classification, summarization, model-graded-eval-for-summarization, reasoning, model-graded-evaluation-reasoning,  )\n",
    "L6_code_llama.ipynb ( from utils import llama, code_llama ) (writing/generating code to solve math problem, code-in-filling - completing partially complted code, recursive fibonacci, fast-cashed-fibonacci, ask code_llama to compare runtimes of two func. implementations, size of a context window, taking longer text in code_llama )\n",
    "Code Llama\n",
    "Here are the names of the Code Llama models provided by Together.ai:\n",
    "\n",
    "togethercomputer/CodeLlama-7b                       <<<<   base\n",
    "togethercomputer/CodeLlama-13b                      <<<<\n",
    "togethercomputer/CodeLlama-34b                      <<<<\n",
    "togethercomputer/CodeLlama-7b-Python                py-specialized\n",
    "togethercomputer/CodeLlama-13b-Python               py-specialized\n",
    "togethercomputer/CodeLlama-34b-Python               py-specialized\n",
    "togethercomputer/CodeLlama-7b-Instruct               <<<<\n",
    "togethercomputer/CodeLlama-13b-Instruct              <<<<    human like output\n",
    "togethercomputer/CodeLlama-34b-Instruct)             <<<<   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on Code Llama's summarization performance\n",
    "Note that while the Code Llama model could handle the longer text, the output here isn't that great - the response is very repetitive.\n",
    "\n",
    "Code Llama's primary skill is writing code.\n",
    "Experiment to see if you can prompt the Code Llama model to improve its output.\n",
    "You may need to trade off performance and input text size depending on your task.\n",
    "You could ask Llama 2 70B chat to help you evaluate how well the Code Llama model is doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Llama-2 on your own computer!\n",
    "Llama-2 is free to download on your own machine!\n",
    "One way to install and use llama on your computer is to go to https://ollama.com/ and download app. It will be like installing a regular application.\n",
    "To use llama-2, the full instructions are here: https://ollama.com/library/llama2\n",
    "Here's an quick summary of how to get started:\n",
    "Follow the installation instructions (for Windows, Mac or Linux).\n",
    "Open the command line interface (CLI) and type ollama run llama2.\n",
    "The first time you do this, it will take some time to download the llama-2 model. After that, you'll see\n",
    ">>> Send a message (/? for help)\n",
    "\n",
    "You can type your prompt and the llama-2 model on your computer will give you a response!\n",
    "To exit, type /bye.\n",
    "For a list of other commands, type /?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the together.ai API url\n",
    "import os\n",
    "url = f\"{os.getenv('DLAI_TOGETHER_API_BASE')}/inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the togerher.ai API key\n",
    "import os\n",
    "together_api_key = os.getenv('TOGETHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store keywords that will be passed to the API\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {together_api_key}\",\n",
    "    \"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model to call\n",
    "model=\"togethercomputer/llama-2-7b-chat\"\n",
    "â€‹prompt = \"\"\"\n",
    "Please write me a birthday card for my dear friend, Andrew.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add instruction tags to the prompt\n",
    "prompt = f\"[INST]{prompt}[/INST]\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temperature and max_tokens\n",
    "temperature = 0.0\n",
    "max_tokens = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"temperature\": temperature,\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post(url,\n",
    "                         headers=headers,\n",
    "                         json=data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response.json()\n",
    "response.json()['output']\n",
    "response.json()['output']['choices']\n",
    "response.json()['output']['choices'][0]\n",
    "response.json()['output']['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the output of the llama helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llama\n",
    "# compare to the output of the helper function\n",
    "llama(prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
